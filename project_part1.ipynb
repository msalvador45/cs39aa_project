{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part 1\n",
    "\n",
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/cs39aa_project/blob/main/project_part1.ipynb)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/cs39aa_project/blob/main/project_part1.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction/Background\n",
    "\n",
    "In this project I will attempting to train a model to predict news media bias. In order to do this I will be utilizing the dataset NLPCSS-20 from the Association for Computational Linguistics which can be found on Github. The dataset is a collection or corpus contrived of 6,964 news articles. The labels of how we describe the corpus are the following:\n",
    "- title: title of news article\n",
    "- content: content of news article\n",
    "- source: source of the content\n",
    "- allsides-bias: bias (left, center or right)\n",
    "- misc: aurhor, date, etc\n",
    "- adfontes_fair: how fair is the news article (bias, neutral or unknown)\n",
    "- adfontes_political: how political bias skewed the article is (bias, neutral or unknown)\n",
    "- event_id:event id\n",
    "\n",
    "This is a text classification problem as how we are trying to classify combination of strings that demonstrate wheter a news content is politically bias or not. The combination of strings is the content itself, it is then labeled with how bias that content is. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "First step is to analyze the data we have in order to figure out how our data looks like and what it is trying to tell us.\n",
    "\n",
    "The dataset comes in a Json file so there are extra steps as to converting the data into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/msalvador45/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/msalvador45/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/msalvador45/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all of the python modules/packages you'll need here\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can oberseve the json data set is not able to be formatted into a dataframe, this is becuase its format is unsupported by pandas and will have to be reformated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert json file to dataframe\n",
    "# path has original data, path2 is a small excerpt from og data w/ a fix to check if it is able to reformatt into df\n",
    "path= \"NLPCSS-20-main/data/released_data.json\"\n",
    "path2= \"NLPCSS-20-main/data/example.json\"\n",
    "\n",
    "# test_df = pd.read_json(path, orient='records')\n",
    "test_df2 = pd.read_json(path2, orient='records')\n",
    "\n",
    "# print(test_df)\n",
    "print (test_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to reformatted json file\n",
    "path_data= 'NLPCSS-20-main/data/data.json'\n",
    "#reformatt json file\n",
    "with open(path, 'r') as rf:\n",
    "    with open(path_data, 'w') as wf:\n",
    "        #add [ at the beginning of the corpus\n",
    "        wf.write('[\\n')\n",
    "        #add a comma at the end of each line\n",
    "        lines= rf.read().splitlines()\n",
    "        new_line= ',\\n'.join(lines)\n",
    "        wf.write(new_line)\n",
    "        #add ] at the end of the corpus\n",
    "        wf.write('\\n]')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now convert data into a pandas dataframe and checkout initial observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert reformatted dataset into dataframe\"\"\"\n",
    "df = pd.read_json(path_data, orient='records')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do some basic pandas data exploration to get a sense of the data.\n",
    "There are 7775 observations with 8 columns, but it seems as we might not have to use all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(\"\\nThe shape of dataset as of right now is \", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to deal with a simple model first so we will proceed to only use 'content' and 'allsides_bias' first, and we can drop every other column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['source','title','event_id','misc','adfontes_fair','adfontes_political'], inplace=True)\n",
    "#lowercase the content column\n",
    "df['content']= df['content'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the shape and columns of our remodified data frame as well as the unique values of our allsides_bias and its value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(\"\\nThe shape of dataset as of right now is \", df.shape,\"\\n\")\n",
    "print(\"The unique values for allsides_bias is:\\n\",df.allsides_bias.unique(),\"\\n\")\n",
    "df.allsides_bias.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see most results come from the left, around 37% com from the right and 15% are center biased.\n",
    "\n",
    "Now lets check for empty values in both columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_entries= df.isnull().sum()\n",
    "null_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our data frame as of now has two rows, and both contain values that are not empty or NaN.\n",
    "\n",
    "Lets take a look into the stopwords vs non-stopwords within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Create a corpus without stopwords\"\n",
    "from nltk.corpus import stopwords\n",
    "stops= set(stopwords.words('english'))\n",
    "\n",
    "corpus=[]\n",
    "new= df['content'].str.split()\n",
    "new= new.values.tolist()\n",
    "corpus=[word for i in new for word in i]\n",
    "\n",
    "from collections import defaultdict\n",
    "dic= defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stops:\n",
    "        dic[word]+=1\n",
    "\n",
    "top= sorted(dic.items(), key=lambda x:x[1], reverse= True)[:10]\n",
    "x,y= zip(*top)\n",
    "print(\"Bar graph of amt of stopwords in content:\\n\")\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"create a chart to show number of occurences of words in dataset\"\n",
    "from collections import Counter\n",
    "\n",
    "counter= Counter(corpus)\n",
    "most= counter.most_common()\n",
    "\n",
    "x,y= [], []\n",
    "for word, count in most[:40]:\n",
    "    if (word not in stops):\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "\n",
    "sns.barplot(x=y, y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making a baseline for the model, we will want to process the dataset we will have to tokenize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Using nltk tokenize the dataset\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "df['tokens_raw']= df['content'].apply(word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as removing stopwords from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "chars2remove= set(['.','!','/','?','#','-']) \n",
    "#stops contains our stopwords\n",
    "df['tokens_raw']= df['tokens_raw'].apply(lambda x: [w for w in x if w not in stops])\n",
    "df['tokens_raw']= df['tokens_raw'].apply(lambda x: [w for w in x if w not in chars2remove])\n",
    "df['tokens_raw']= df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^http',w)])\n",
    "df['tokens_raw']= df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^@', w)])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will proceed to lemmetize our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lemmatize() got an unexpected keyword argument 'ps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m \u001b[39mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m      3\u001b[0m lemmatizer\u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 4\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtokens_raw\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: [lemmatizer\u001b[39m.\u001b[39;49mlemmatize(w, ps\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mv\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m w \u001b[39min\u001b[39;49;00m x])\n\u001b[1;32m      5\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpProj/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpProj/lib/python3.9/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpProj/lib/python3.9/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1144\u001b[0m             values,\n\u001b[1;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1147\u001b[0m         )\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpProj/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn [16], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m \u001b[39mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m      3\u001b[0m lemmatizer\u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 4\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtokens_raw\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: [lemmatizer\u001b[39m.\u001b[39mlemmatize(w, ps\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m x])\n\u001b[1;32m      5\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn [16], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m \u001b[39mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m      3\u001b[0m lemmatizer\u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 4\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtokens_raw\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: [lemmatizer\u001b[39m.\u001b[39;49mlemmatize(w, ps\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mv\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m x])\n\u001b[1;32m      5\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "\u001b[0;31mTypeError\u001b[0m: lemmatize() got an unexpected keyword argument 'ps'"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "df['tokens']= df['tokens_raw'].apply(lambda x: [lemmatizer.lemmatize(w, pos=\"v\") for w in x])\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b34940b68f42765500b06a5bff1526c43e9cf16917429620a7b1b96d08e5bb48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
